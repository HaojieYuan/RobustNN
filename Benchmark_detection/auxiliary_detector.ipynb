{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from utils import cifar10_load\n",
    "from models import build_ResNet\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_dev, y_dev), (x_test, y_test) = cifar10_load()\n",
    "\n",
    "# 这里用name_scope是无效的\n",
    "model = tf.keras.models.load_model('ResNet32_acc893.h5')\n",
    "base_varlist = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lr_init 0.1, 40epochs后调为0.01, 60epochs后调为0.001, 共100epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Activation\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# 这里name_scope是有效的\n",
    "detector = Sequential([\n",
    "    Conv2D(96, (3,3), strides=(1,1), padding='same', activation='relu', kernel_initializer='he_normal', input_shape=(32,32,16)),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(192, (3,3), strides=(1,1), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(192, (3,3), strides=(1,1), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "    Conv2D(2, (1,1), strides=(1,1), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "    GlobalAveragePooling2D()])\n",
    "detect_pipeline = Model(inputs=model.input, outputs=Activation('softmax', name=\"detect_softmax\")(detector(model.get_layer('activation_10').output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/robustnn_workspace/cleverhans-master/cleverhans/attacks_tf.py:27: UserWarning: attacks_tf is deprecated and will be removed on 2019-07-18 or after. Code should import functions from their new locations directly.\n",
      "  warnings.warn(\"attacks_tf is deprecated and will be removed on 2019-07-18\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tf/robustnn_workspace/Benchmark_detection/modified_PGD.py:94: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/robustnn_workspace/cleverhans-master/cleverhans/compat.py:23: UserWarning: <function reduce_max_v1 at 0x7f722d1bbea0> is deprecated. Switch to calling the equivalent function in tensorflow.  This function was originally needed as a compatibility layer for old versions of tensorflow,  but support for those versions has now been dropped.\n",
      "  warnings.warn(str(f) + \" is deprecated. Switch to calling the equivalent function in tensorflow. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tf/robustnn_workspace/cleverhans-master/cleverhans/compat.py:80: calling softmax_cross_entropy_with_logits_v2_helper (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/robustnn_workspace/cleverhans-master/cleverhans/compat.py:23: UserWarning: <function reduce_sum_v1 at 0x7f722d1bb1e0> is deprecated. Switch to calling the equivalent function in tensorflow.  This function was originally needed as a compatibility layer for old versions of tensorflow,  but support for those versions has now been dropped.\n",
      "  warnings.warn(str(f) + \" is deprecated. Switch to calling the equivalent function in tensorflow. \"\n"
     ]
    }
   ],
   "source": [
    "from cleverhans.utils_keras import KerasModelWrapper\n",
    "from modified_PGD import ProjectedGradientDescent\n",
    "import numpy as np\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(None,32,32,3))\n",
    "x_label = tf.placeholder(tf.float32, shape=(None,2))\n",
    "adv_label = tf.placeholder(tf.float32, shape=(None,2))\n",
    "\n",
    "model_wrap = KerasModelWrapper(model)\n",
    "detect_wrap = KerasModelWrapper(detect_pipeline)\n",
    "pgd_wrap = ProjectedGradientDescent(model_wrap, detect_wrap)\n",
    "\n",
    "pgd_params = {'eps':0.3, 'eps_iter':0.25, 'nb_iter':10, 'y':adv_label, 'ord':np.inf,\n",
    "              'clip_min':0, 'clip_max':1, 'y_target':None, 'rand_init':None, 'rand_minmax':0.3, 'sanity_checks':True}\n",
    "\n",
    "adv_x = pgd_wrap.generate(x, **pgd_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_labels(length):\n",
    "    adv_labels = np.zeros([length, 2])\n",
    "    normal_labels = np.copy(adv_labels)\n",
    "    adv_labels[:, 1] = 1 \n",
    "    normal_labels[:, 0] = 1\n",
    "    \n",
    "    return adv_labels, normal_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_uninitialized(sess):\n",
    "       global_vars = tf.global_variables()\n",
    "       is_not_initialized = sess.run([tf.is_variable_initialized(var) for var in global_vars])\n",
    "       not_initialized_vars = [v for (v, f) in zip(global_vars, is_not_initialized) if not f]\n",
    "       #print([str(i.name) for i in not_initialized_vars]) # only for testing\n",
    "       if len(not_initialized_vars):\n",
    "            sess.run(tf.variables_initializer(not_initialized_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_varlist = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "new_varlist = list(set(total_varlist) - set(base_varlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['beta1_power:0', 'beta2_power:0', 'conv2d_2_1/bias/Adam:0', 'conv2d_2_1/bias/Adam_1:0', 'conv2d_3_1/bias/Adam:0', 'conv2d_3_1/bias/Adam_1:0', 'conv2d_1_1/bias/Adam:0', 'conv2d_1_1/bias/Adam_1:0', 'conv2d_2_1/kernel/Adam:0', 'conv2d_2_1/kernel/Adam_1:0', 'conv2d_33/kernel/Adam:0', 'conv2d_33/kernel/Adam_1:0', 'conv2d_1_1/kernel/Adam:0', 'conv2d_1_1/kernel/Adam_1:0', 'conv2d_33/bias/Adam:0', 'conv2d_33/bias/Adam_1:0', 'conv2d_3_1/kernel/Adam:0', 'conv2d_3_1/kernel/Adam_1:0']\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "training_epochs = 100\n",
    "\n",
    "detect_x = detect_pipeline(x)\n",
    "detect_adv = detect_pipeline(adv_x)\n",
    "\n",
    "sess = tf.keras.backend.get_session()\n",
    "\n",
    "loss1 = tf.keras.backend.categorical_crossentropy(x_label, detect_x)\n",
    "loss2 = tf.keras.backend.categorical_crossentropy(adv_label, detect_adv)\n",
    "total_loss = loss1 + loss2\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(total_loss, var_list=new_varlist)\n",
    "\n",
    "correct_prediction1 = tf.equal(tf.argmax(detect_x, 1), tf.argmax(x_label, 1))\n",
    "correct_prediction2 = tf.equal(tf.argmax(detect_adv, 1), tf.argmax(adv_label, 1))\n",
    "accuracy1 = tf.reduce_mean(tf.cast(correct_prediction1, \"float\"), name='accuracy')\n",
    "accuracy2 = tf.reduce_mean(tf.cast(correct_prediction2, \"float\"), name='accuracy')\n",
    "accuracy = 0.5*accuracy1 + 0.5*accuracy2\n",
    "\n",
    "initialize_uninitialized(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0, test accuracy: 0.993\n",
      "iteration: 1, test accuracy: 1\n",
      "iteration: 2, test accuracy: 1\n",
      "iteration: 3, test accuracy: 1\n",
      "iteration: 4, test accuracy: 1\n",
      "iteration: 5, test accuracy: 1\n",
      "iteration: 6, test accuracy: 1\n",
      "iteration: 7, test accuracy: 1\n",
      "iteration: 8, test accuracy: 1\n",
      "iteration: 9, test accuracy: 1\n",
      "iteration: 10, test accuracy: 1\n",
      "iteration: 11, test accuracy: 1\n",
      "iteration: 12, test accuracy: 1\n",
      "iteration: 13, test accuracy: 1\n",
      "iteration: 14, test accuracy: 1\n",
      "iteration: 15, test accuracy: 1\n",
      "iteration: 16, test accuracy: 1\n",
      "iteration: 17, test accuracy: 1\n",
      "iteration: 18, test accuracy: 1\n",
      "iteration: 19, test accuracy: 1\n",
      "iteration: 20, test accuracy: 1\n",
      "iteration: 21, test accuracy: 1\n",
      "iteration: 22, test accuracy: 1\n",
      "iteration: 23, test accuracy: 1\n",
      "iteration: 24, test accuracy: 1\n",
      "iteration: 25, test accuracy: 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-fadac8a36abf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         train_step.run(session=sess, feed_dict={x: x_train[j*batch_size:(j+1)*batch_size],\n\u001b[0;32m----> 5\u001b[0;31m                                                x_label: normal_labels, adv_label: adv_labels})\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0madv_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormal_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_dev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     print(\"iteration: %d, test accuracy: %g\" % (i, accuracy.eval(session=sess, feed_dict={x: x_dev[:500], \n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m   2448\u001b[0m         \u001b[0mnone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0msession\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2449\u001b[0m     \"\"\"\n\u001b[0;32m-> 2450\u001b[0;31m     \u001b[0m_run_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2452\u001b[0m \u001b[0m_gradient_registry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRegistry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gradient\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_run_using_default_session\u001b[0;34m(operation, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   5214\u001b[0m                        \u001b[0;34m\"the operation's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5215\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 5216\u001b[0;31m   \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(training_epochs):\n",
    "    adv_labels, normal_labels = generate_labels(batch_size)\n",
    "    for j in range(len(x_train)//batch_size-1):\n",
    "        train_step.run(session=sess, feed_dict={x: x_train[j*batch_size:(j+1)*batch_size],\n",
    "                                               x_label: normal_labels, adv_label: adv_labels})\n",
    "    adv_labels, normal_labels = generate_labels(len(x_dev[:500]))\n",
    "    print(\"iteration: %d, test accuracy: %g\" % (i, accuracy.eval(session=sess, feed_dict={x: x_dev[:500], \n",
    "                                                                                          x_label: normal_labels, \n",
    "                                                                                          adv_label: adv_labels})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_pipeline.save('detector.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlayers = [layer.get_output_at(-1) for layer in detect_pipeline.layers]\n",
    "test = Model(inputs=detect_pipeline.input, outputs=outlayers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将Dectector和原来model的隐藏层连起来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 32, 32, 16)   448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1 (BatchNo (None, 32, 32, 16)   64          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 32, 32, 16)   0           batch_normalization_v1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 16)   2320        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_1 (Batch (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_v1_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_2 (Batch (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 32, 32, 16)   0           activation[0][0]                 \n",
      "                                                                 batch_normalization_v1_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 16)   0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_3 (Batch (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 16)   0           batch_normalization_v1_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 16)   2320        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_4 (Batch (None, 32, 32, 16)   64          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 16)   0           activation_2[0][0]               \n",
      "                                                                 batch_normalization_v1_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 16)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 16)   2320        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_5 (Batch (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 16)   0           batch_normalization_v1_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2320        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_6 (Batch (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 16)   0           activation_4[0][0]               \n",
      "                                                                 batch_normalization_v1_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 16)   0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 16)   2320        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_7 (Batch (None, 32, 32, 16)   64          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 16)   0           batch_normalization_v1_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 16)   2320        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_8 (Batch (None, 32, 32, 16)   64          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 16)   0           activation_6[0][0]               \n",
      "                                                                 batch_normalization_v1_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 16)   0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 32, 16)   2320        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_9 (Batch (None, 32, 32, 16)   64          conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 32, 32, 16)   0           batch_normalization_v1_9[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 16)   2320        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_10 (Batc (None, 32, 32, 16)   64          conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 32, 32, 16)   0           activation_8[0][0]               \n",
      "                                                                 batch_normalization_v1_10[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 16)   0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None, 2)            512354      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "detect_softmax (Activation)     (None, 2)            0           sequential[1][0]                 \n",
      "==================================================================================================\n",
      "Total params: 536,706\n",
      "Trainable params: 512,354\n",
      "Non-trainable params: 24,352\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "detect_pipeline.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleverhans.utils_keras import KerasModelWrapper\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(None,32,32,3))\n",
    "y = tf.placeholder(tf.float32, shape=(None,10))\n",
    "model_wrap = KerasModelWrapper(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Activation\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "hidden = model.get_layer('activation_10').output\n",
    "\n",
    "conv1 = Conv2D(96, (3,3), strides=(1,1), padding='same', activation='relu', kernel_initializer='uniform',\n",
    "              name='detector_conv1')(hidden)\n",
    "mp1 = MaxPooling2D(name='detector_mp1')(conv1)\n",
    "conv2 = Conv2D(192, (3,3), strides=(1,1), padding='same', activation='relu', kernel_initializer='uniform',\n",
    "              name='detector_conv2')(mp1)\n",
    "mp2 = MaxPooling2D(name='detector_mp2')(conv2)\n",
    "conv3 = Conv2D(192, (3,3), strides=(1,1), padding='same', activation='relu', kernel_initializer='uniform',\n",
    "              name='detector_conv3')(mp2)\n",
    "conv4 = Conv2D(2, (1,1), strides=(1,1), padding='same', activation='relu', kernel_initializer='uniform',\n",
    "              name='detector_conv4')(conv3)\n",
    "GAP = GlobalAveragePooling2D(name='detector_GAP')(conv4)\n",
    "\n",
    "detector_out = Activation('softmax', name='detector_softmax')(GAP)\n",
    "detect_pipeline = Model(inputs=model.input, outputs=detector_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "detect_pipeline = Model(inputs=model.input, outputs=Activation('softmax', name=\"detect_softmax\")(detector(model.get_layer('add_3').output)))\n",
    "detect_wrap = KerasModelWrapper(detect_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 32, 32, 16)   448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1 (BatchNo (None, 32, 32, 16)   64          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 32, 32, 16)   0           batch_normalization_v1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 16)   2320        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_1 (Batch (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_v1_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_2 (Batch (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 32, 32, 16)   0           activation[0][0]                 \n",
      "                                                                 batch_normalization_v1_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 16)   0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_3 (Batch (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 16)   0           batch_normalization_v1_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 16)   2320        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_4 (Batch (None, 32, 32, 16)   64          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 16)   0           activation_2[0][0]               \n",
      "                                                                 batch_normalization_v1_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 16)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 16)   2320        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_5 (Batch (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 16)   0           batch_normalization_v1_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2320        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_6 (Batch (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 16)   0           activation_4[0][0]               \n",
      "                                                                 batch_normalization_v1_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 16)   0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 16)   2320        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_7 (Batch (None, 32, 32, 16)   64          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 16)   0           batch_normalization_v1_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 16)   2320        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_8 (Batch (None, 32, 32, 16)   64          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 16)   0           activation_6[0][0]               \n",
      "                                                                 batch_normalization_v1_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None, 2)            512354      add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "detect_softmax (Activation)     (None, 2)            0           sequential[1][0]                 \n",
      "==================================================================================================\n",
      "Total params: 531,938\n",
      "Trainable params: 512,354\n",
      "Non-trainable params: 19,584\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "detect_pipeline.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.49706942, 0.5029305 ]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import cifar10_load\n",
    "(x_train, y_train), (x_dev, y_dev), (x_test, y_test) = cifar10_load()\n",
    "import numpy as np\n",
    "detect_pipeline.predict(x_dev[0].reshape(-1,32,32,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_pipeline.save('pipeline_test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "detect_pipeline = tf.keras.models.load_model('pipeline_test.h5')\n",
    "outlayers = [layer.output for layer in detect_pipeline.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'global_average_pooling2d/Mean:0' shape=(?, 2) dtype=float32>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outlayers[31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Model(inputs=detect_pipeline.input, outputs=outlayers[32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检测器对对抗样本输出为1,正常样本输出为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/robustnn_workspace/cleverhans-master/cleverhans/attacks_tf.py:27: UserWarning: attacks_tf is deprecated and will be removed on 2019-07-18 or after. Code should import functions from their new locations directly.\n",
      "  warnings.warn(\"attacks_tf is deprecated and will be removed on 2019-07-18\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tf/robustnn_workspace/Benchmark_detection/modified_PGD.py:94: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/robustnn_workspace/cleverhans-master/cleverhans/compat.py:23: UserWarning: <function reduce_max_v1 at 0x7f87a1f6bea0> is deprecated. Switch to calling the equivalent function in tensorflow.  This function was originally needed as a compatibility layer for old versions of tensorflow,  but support for those versions has now been dropped.\n",
      "  warnings.warn(str(f) + \" is deprecated. Switch to calling the equivalent function in tensorflow. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tf/robustnn_workspace/cleverhans-master/cleverhans/compat.py:80: calling softmax_cross_entropy_with_logits_v2_helper (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/robustnn_workspace/cleverhans-master/cleverhans/compat.py:23: UserWarning: <function reduce_sum_v1 at 0x7f87a1f6b1e0> is deprecated. Switch to calling the equivalent function in tensorflow.  This function was originally needed as a compatibility layer for old versions of tensorflow,  but support for those versions has now been dropped.\n",
      "  warnings.warn(str(f) + \" is deprecated. Switch to calling the equivalent function in tensorflow. \"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Graph disconnected: cannot obtain value for tensor Tensor(\"conv2d_input:0\", shape=(?, 32, 32, 16), dtype=float32) at layer \"conv2d_input\". The following previous layers were accessed without issue: ['input_1', 'conv2d', 'batch_normalization_v1', 'activation', 'conv2d_1', 'batch_normalization_v1_1', 'activation_1', 'conv2d_2', 'batch_normalization_v1_2', 'add', 'activation_2', 'conv2d_3', 'batch_normalization_v1_3', 'activation_3', 'conv2d_4', 'batch_normalization_v1_4', 'add_1', 'activation_4', 'conv2d_5', 'batch_normalization_v1_5', 'activation_5', 'conv2d_6', 'batch_normalization_v1_6', 'add_2', 'activation_6']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-43f14723ccd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m pgd_params = {'eps':0.3, 'eps_iter':0.25, 'nb_iter':10, 'y':np.array([[0, 1]]), 'ord':np.inf,\n\u001b[1;32m      6\u001b[0m               'clip_min':0, 'clip_max':1, 'y_target':None, 'rand_init':None, 'rand_minmax':0.3, 'sanity_checks':True}\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0madv_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpgd_wrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpgd_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#train_pipeline = Model(inputs=x, outputs=[model(x), model(adv_x), detect_pipeline(x), detect_pipline(adv_x)])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tf/robustnn_workspace/Benchmark_detection/modified_PGD.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     _, adv_x = tf.while_loop(cond, body, (tf.zeros([]), adv_x), back_prop=True,\n\u001b[0;32m--> 168\u001b[0;31m                              maximum_iterations=self.nb_iter)\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;31m# Asserts run only on CPU.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m   3554\u001b[0m       \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWHILE_CONTEXT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3555\u001b[0m     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants,\n\u001b[0;32m-> 3556\u001b[0;31m                                     return_same_structure)\n\u001b[0m\u001b[1;32m   3557\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmaximum_iterations\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3558\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mBuildLoop\u001b[0;34m(self, pred, body, loop_vars, shape_invariants, return_same_structure)\u001b[0m\n\u001b[1;32m   3085\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mutation_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3086\u001b[0m         original_body_result, exit_vars = self._BuildLoop(\n\u001b[0;32m-> 3087\u001b[0;31m             pred, body, original_loop_vars, loop_vars, shape_invariants)\n\u001b[0m\u001b[1;32m   3088\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3089\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36m_BuildLoop\u001b[0;34m(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\u001b[0m\n\u001b[1;32m   3020\u001b[0m         flat_sequence=vars_for_body_with_tensor_arrays)\n\u001b[1;32m   3021\u001b[0m     \u001b[0mpre_summaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3022\u001b[0;31m     \u001b[0mbody_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpacked_vars_for_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3023\u001b[0m     \u001b[0mpost_summaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3024\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(i, lv)\u001b[0m\n\u001b[1;32m   3523\u001b[0m         cond = lambda i, lv: (  # pylint: disable=g-long-lambda\n\u001b[1;32m   3524\u001b[0m             math_ops.logical_and(i < maximum_iterations, orig_cond(*lv)))\n\u001b[0;32m-> 3525\u001b[0;31m         \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3527\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tf/robustnn_workspace/Benchmark_detection/modified_PGD.py\u001b[0m in \u001b[0;36mbody\u001b[0;34m(i, adv_x)\u001b[0m\n\u001b[1;32m    149\u001b[0m       \u001b[0malhpa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m       \u001b[0madv_base\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFGM1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madv_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfgm1_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m       \u001b[0madv_detect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFGM2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madv_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfgm2_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m       \u001b[0madv_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0madv_base\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0madv_detect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tf/robustnn_workspace/cleverhans-master/cleverhans/attacks/fast_gradient_method.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m     return fgm(\n\u001b[1;32m     53\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tf/robustnn_workspace/cleverhans-master/cleverhans/utils_keras.py\u001b[0m in \u001b[0;36mget_logits\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \"\"\"\n\u001b[1;32m    152\u001b[0m     \u001b[0mlogits_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_logits_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0mlogits_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;31m# Need to deal with the case where softmax is part of the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tf/robustnn_workspace/cleverhans-master/cleverhans/utils_keras.py\u001b[0m in \u001b[0;36mget_layer\u001b[0;34m(self, x, layer)\u001b[0m\n\u001b[1;32m    217\u001b[0m     \"\"\"\n\u001b[1;32m    218\u001b[0m     \u001b[0;31m# Return the symbolic representation for this layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m       \u001b[0mrequested\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tf/robustnn_workspace/cleverhans-master/cleverhans/utils_keras.py\u001b[0m in \u001b[0;36mfprop\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0;31m# Make a new model that returns each of the layers as output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m       \u001b[0mout_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKerasModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;31m# and get the outputs for that model on the input x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0;31m# Create a cache for iterator get_next op.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_get_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWeakKeyDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m         'inputs' in kwargs and 'outputs' in kwargs):\n\u001b[1;32m     80\u001b[0m       \u001b[0;31m# Graph network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m       \u001b[0;31m# Subclassed network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/checkpointable/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m       \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_init_graph_network\u001b[0;34m(self, inputs, outputs, name)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;31m# Keep track of the network's nodes and layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     nodes, nodes_by_depth, layers, layers_by_depth = _map_graph_network(\n\u001b[0;32m--> 221\u001b[0;31m         self.inputs, self.outputs)\n\u001b[0m\u001b[1;32m    222\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_network_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes_by_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes_by_depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_map_graph_network\u001b[0;34m(inputs, outputs)\u001b[0m\n\u001b[1;32m   1850\u001b[0m                              \u001b[0;34m'The following previous layers '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m                              \u001b[0;34m'were accessed without issue: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1852\u001b[0;31m                              str(layers_with_complete_input))\n\u001b[0m\u001b[1;32m   1853\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1854\u001b[0m           \u001b[0mcomputable_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Graph disconnected: cannot obtain value for tensor Tensor(\"conv2d_input:0\", shape=(?, 32, 32, 16), dtype=float32) at layer \"conv2d_input\". The following previous layers were accessed without issue: ['input_1', 'conv2d', 'batch_normalization_v1', 'activation', 'conv2d_1', 'batch_normalization_v1_1', 'activation_1', 'conv2d_2', 'batch_normalization_v1_2', 'add', 'activation_2', 'conv2d_3', 'batch_normalization_v1_3', 'activation_3', 'conv2d_4', 'batch_normalization_v1_4', 'add_1', 'activation_4', 'conv2d_5', 'batch_normalization_v1_5', 'activation_5', 'conv2d_6', 'batch_normalization_v1_6', 'add_2', 'activation_6']"
     ]
    }
   ],
   "source": [
    "from modified_PGD import ProjectedGradientDescent\n",
    "import numpy as np\n",
    "\n",
    "pgd_wrap = ProjectedGradientDescent(model_wrap, detect_wrap)\n",
    "pgd_params = {'eps':0.3, 'eps_iter':0.25, 'nb_iter':10, 'y':np.array([[0, 1]]), 'ord':np.inf,\n",
    "              'clip_min':0, 'clip_max':1, 'y_target':None, 'rand_init':None, 'rand_minmax':0.3, 'sanity_checks':True}\n",
    "adv_x = pgd_wrap.generate(x, **pgd_params)\n",
    "\n",
    "#train_pipeline = Model(inputs=x, outputs=[model(x), model(adv_x), detect_pipeline(x), detect_pipline(adv_x)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train pipeline没有办法抽象成Model储存，只能手写训练流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 2s 483us/sample - loss: 0.4888 - acc: 0.8930\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4887514706611633, 0.893]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = model_wrap.get_layer(x, 'activation_10')\n",
    "detector_out = detector(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 32, 32, 16)   448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1 (BatchNo (None, 32, 32, 16)   64          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 32, 32, 16)   0           batch_normalization_v1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 16)   2320        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_1 (Batch (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_v1_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_2 (Batch (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 32, 32, 16)   0           activation[0][0]                 \n",
      "                                                                 batch_normalization_v1_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 16)   0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_3 (Batch (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 16)   0           batch_normalization_v1_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 16)   2320        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_4 (Batch (None, 32, 32, 16)   64          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 16)   0           activation_2[0][0]               \n",
      "                                                                 batch_normalization_v1_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 16)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 16)   2320        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_5 (Batch (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 16)   0           batch_normalization_v1_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2320        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_6 (Batch (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 16)   0           activation_4[0][0]               \n",
      "                                                                 batch_normalization_v1_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 16)   0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 16)   2320        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_7 (Batch (None, 32, 32, 16)   64          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 16)   0           batch_normalization_v1_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 16)   2320        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_8 (Batch (None, 32, 32, 16)   64          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 16)   0           activation_6[0][0]               \n",
      "                                                                 batch_normalization_v1_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 16)   0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 32, 16)   2320        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_9 (Batch (None, 32, 32, 16)   64          conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 32, 32, 16)   0           batch_normalization_v1_9[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 16)   2320        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_10 (Batc (None, 32, 32, 16)   64          conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 32, 32, 16)   0           activation_8[0][0]               \n",
      "                                                                 batch_normalization_v1_10[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 16)   0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 32)   4640        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_11 (Batc (None, 16, 16, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 32)   0           batch_normalization_v1_11[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 32)   9248        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 32)   544         activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_12 (Batc (None, 16, 16, 32)   128         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 16, 32)   0           conv2d_13[0][0]                  \n",
      "                                                                 batch_normalization_v1_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 32)   0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 16, 32)   9248        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_13 (Batc (None, 16, 16, 32)   128         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 32)   0           batch_normalization_v1_13[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 16, 16, 32)   9248        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_14 (Batc (None, 16, 16, 32)   128         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 16, 16, 32)   0           activation_12[0][0]              \n",
      "                                                                 batch_normalization_v1_14[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 16, 16, 32)   0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 16, 16, 32)   9248        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_15 (Batc (None, 16, 16, 32)   128         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 16, 16, 32)   0           batch_normalization_v1_15[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 16, 16, 32)   9248        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_16 (Batc (None, 16, 16, 32)   128         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 16, 16, 32)   0           activation_14[0][0]              \n",
      "                                                                 batch_normalization_v1_16[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 16, 16, 32)   0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 16, 16, 32)   9248        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_17 (Batc (None, 16, 16, 32)   128         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 16, 16, 32)   0           batch_normalization_v1_17[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 16, 16, 32)   9248        activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_18 (Batc (None, 16, 16, 32)   128         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 16, 16, 32)   0           activation_16[0][0]              \n",
      "                                                                 batch_normalization_v1_18[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16, 16, 32)   0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 16, 16, 32)   9248        activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_19 (Batc (None, 16, 16, 32)   128         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 16, 16, 32)   0           batch_normalization_v1_19[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 16, 16, 32)   9248        activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_20 (Batc (None, 16, 16, 32)   128         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 16, 16, 32)   0           activation_18[0][0]              \n",
      "                                                                 batch_normalization_v1_20[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 16, 16, 32)   0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 8, 8, 64)     18496       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_21 (Batc (None, 8, 8, 64)     256         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 8, 8, 64)     0           batch_normalization_v1_21[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 8, 8, 64)     36928       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 8, 8, 64)     2112        activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_22 (Batc (None, 8, 8, 64)     256         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 8, 8, 64)     0           conv2d_24[0][0]                  \n",
      "                                                                 batch_normalization_v1_22[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 8, 8, 64)     0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 8, 8, 64)     36928       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_23 (Batc (None, 8, 8, 64)     256         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 8, 8, 64)     0           batch_normalization_v1_23[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 8, 8, 64)     36928       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_24 (Batc (None, 8, 8, 64)     256         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 8, 8, 64)     0           activation_22[0][0]              \n",
      "                                                                 batch_normalization_v1_24[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 8, 8, 64)     0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 8, 8, 64)     36928       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_25 (Batc (None, 8, 8, 64)     256         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 8, 8, 64)     0           batch_normalization_v1_25[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 8, 8, 64)     36928       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_26 (Batc (None, 8, 8, 64)     256         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 8, 8, 64)     0           activation_24[0][0]              \n",
      "                                                                 batch_normalization_v1_26[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 8, 8, 64)     0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 8, 8, 64)     36928       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_27 (Batc (None, 8, 8, 64)     256         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 8, 8, 64)     0           batch_normalization_v1_27[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 8, 8, 64)     36928       activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_28 (Batc (None, 8, 8, 64)     256         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 8, 8, 64)     0           activation_26[0][0]              \n",
      "                                                                 batch_normalization_v1_28[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 8, 8, 64)     0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 8, 8, 64)     36928       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_29 (Batc (None, 8, 8, 64)     256         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 8, 8, 64)     0           batch_normalization_v1_29[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 8, 8, 64)     36928       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_30 (Batc (None, 8, 8, 64)     256         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 8, 8, 64)     0           activation_28[0][0]              \n",
      "                                                                 batch_normalization_v1_30[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 8, 8, 64)     0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 1, 1, 64)     0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 64)           0           average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 10)           650         flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 470,218\n",
      "Trainable params: 467,946\n",
      "Non-trainable params: 2,272\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 96)        13920     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 192)       166080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 192)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 192)         331968    \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 2)           386       \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 512,354\n",
      "Trainable params: 512,354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "detector.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 32, 32, 16)   448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1 (BatchNo (None, 32, 32, 16)   64          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 32, 32, 16)   0           batch_normalization_v1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 16)   2320        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_1 (Batch (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_v1_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_2 (Batch (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 32, 32, 16)   0           activation[0][0]                 \n",
      "                                                                 batch_normalization_v1_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 16)   0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_3 (Batch (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 16)   0           batch_normalization_v1_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 16)   2320        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_4 (Batch (None, 32, 32, 16)   64          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 16)   0           activation_2[0][0]               \n",
      "                                                                 batch_normalization_v1_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 16)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 16)   2320        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_5 (Batch (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 16)   0           batch_normalization_v1_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2320        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_6 (Batch (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 16)   0           activation_4[0][0]               \n",
      "                                                                 batch_normalization_v1_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 16)   0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 16)   2320        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_7 (Batch (None, 32, 32, 16)   64          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 16)   0           batch_normalization_v1_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 16)   2320        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_8 (Batch (None, 32, 32, 16)   64          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 16)   0           activation_6[0][0]               \n",
      "                                                                 batch_normalization_v1_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 16)   0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 32, 16)   2320        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_9 (Batch (None, 32, 32, 16)   64          conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 32, 32, 16)   0           batch_normalization_v1_9[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 16)   2320        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_10 (Batc (None, 32, 32, 16)   64          conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 32, 32, 16)   0           activation_8[0][0]               \n",
      "                                                                 batch_normalization_v1_10[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 16)   0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       (None, 2)            512354      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 2)            0           sequential_2[3][0]               \n",
      "==================================================================================================\n",
      "Total params: 536,706\n",
      "Trainable params: 512,354\n",
      "Non-trainable params: 24,352\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "detect_pipeline.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('base_model'):\n",
    "    model = build_ResNet(depth=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "5000/5000 [==============================] - 2s 406us/sample - loss: 2.8628 - acc: 0.2458\n",
      "1407/1407 [==============================] - 42s 30ms/step - loss: 2.4361 - acc: 0.2191 - val_loss: 2.8632 - val_acc: 0.2458\n",
      "Epoch 2/20\n",
      "5000/5000 [==============================] - 2s 341us/sample - loss: 1.8606 - acc: 0.4438\n",
      "1407/1407 [==============================] - 40s 28ms/step - loss: 1.8852 - acc: 0.3867 - val_loss: 1.8628 - val_acc: 0.4438\n",
      "Epoch 3/20\n",
      "5000/5000 [==============================] - 2s 350us/sample - loss: 1.5590 - acc: 0.5220\n",
      "1407/1407 [==============================] - 40s 28ms/step - loss: 1.5862 - acc: 0.5012 - val_loss: 1.5584 - val_acc: 0.5220\n",
      "Epoch 4/20\n",
      "5000/5000 [==============================] - 2s 335us/sample - loss: 1.5672 - acc: 0.5258\n",
      "1407/1407 [==============================] - 40s 28ms/step - loss: 1.4372 - acc: 0.5548 - val_loss: 1.5674 - val_acc: 0.5258\n",
      "Epoch 5/20\n",
      "5000/5000 [==============================] - 2s 338us/sample - loss: 1.6966 - acc: 0.5288\n",
      "1407/1407 [==============================] - 40s 29ms/step - loss: 1.3488 - acc: 0.5901 - val_loss: 1.6992 - val_acc: 0.5288\n",
      "Epoch 6/20\n",
      "5000/5000 [==============================] - 2s 326us/sample - loss: 1.5990 - acc: 0.5820\n",
      "1407/1407 [==============================] - 40s 28ms/step - loss: 1.2744 - acc: 0.6233 - val_loss: 1.5964 - val_acc: 0.5820\n",
      "Epoch 7/20\n",
      "5000/5000 [==============================] - 2s 349us/sample - loss: 1.2147 - acc: 0.6548\n",
      "1407/1407 [==============================] - 40s 28ms/step - loss: 1.2114 - acc: 0.6525 - val_loss: 1.2162 - val_acc: 0.6548\n",
      "Epoch 8/20\n",
      "5000/5000 [==============================] - 2s 352us/sample - loss: 1.3687 - acc: 0.6224\n",
      "1407/1407 [==============================] - 40s 29ms/step - loss: 1.1581 - acc: 0.6782 - val_loss: 1.3719 - val_acc: 0.6224\n",
      "Epoch 9/20\n",
      "5000/5000 [==============================] - 2s 345us/sample - loss: 1.4622 - acc: 0.6044\n",
      "1407/1407 [==============================] - 40s 28ms/step - loss: 1.1351 - acc: 0.6875 - val_loss: 1.4609 - val_acc: 0.6044\n",
      "Epoch 10/20\n",
      "5000/5000 [==============================] - 2s 351us/sample - loss: 2.1743 - acc: 0.4950\n",
      "1407/1407 [==============================] - 40s 29ms/step - loss: 1.1102 - acc: 0.7032 - val_loss: 2.1736 - val_acc: 0.4950\n",
      "Epoch 11/20\n",
      "5000/5000 [==============================] - 2s 338us/sample - loss: 1.6706 - acc: 0.5496\n",
      "1407/1407 [==============================] - 40s 29ms/step - loss: 1.1006 - acc: 0.7100 - val_loss: 1.6720 - val_acc: 0.5496\n",
      "Epoch 12/20\n",
      "5000/5000 [==============================] - 2s 354us/sample - loss: 1.1639 - acc: 0.6900\n",
      "1407/1407 [==============================] - 40s 29ms/step - loss: 1.0828 - acc: 0.7172 - val_loss: 1.1641 - val_acc: 0.6900\n",
      "Epoch 13/20\n",
      "5000/5000 [==============================] - 2s 346us/sample - loss: 1.3199 - acc: 0.6558\n",
      "1407/1407 [==============================] - 40s 29ms/step - loss: 1.0749 - acc: 0.7217 - val_loss: 1.3227 - val_acc: 0.6558\n",
      "Epoch 14/20\n",
      "5000/5000 [==============================] - 2s 351us/sample - loss: 1.3015 - acc: 0.6756\n",
      "1407/1407 [==============================] - 40s 28ms/step - loss: 1.0695 - acc: 0.7276 - val_loss: 1.3037 - val_acc: 0.6756\n",
      "Epoch 15/20\n",
      "5000/5000 [==============================] - 2s 331us/sample - loss: 1.1074 - acc: 0.7252\n",
      "1407/1407 [==============================] - 40s 28ms/step - loss: 1.0630 - acc: 0.7349 - val_loss: 1.1100 - val_acc: 0.7252\n",
      "Epoch 16/20\n",
      "5000/5000 [==============================] - 2s 322us/sample - loss: 1.3159 - acc: 0.6636\n",
      "1407/1407 [==============================] - 39s 28ms/step - loss: 1.0534 - acc: 0.7377 - val_loss: 1.3166 - val_acc: 0.6636\n",
      "Epoch 17/20\n",
      "5000/5000 [==============================] - 2s 350us/sample - loss: 1.1734 - acc: 0.7122\n",
      "1407/1407 [==============================] - 40s 28ms/step - loss: 1.0508 - acc: 0.7405 - val_loss: 1.1785 - val_acc: 0.7122\n",
      "Epoch 18/20\n",
      "5000/5000 [==============================] - 2s 320us/sample - loss: 1.1083 - acc: 0.7248\n",
      "1407/1407 [==============================] - 40s 29ms/step - loss: 1.0413 - acc: 0.7450 - val_loss: 1.1086 - val_acc: 0.7248\n",
      "Epoch 19/20\n",
      "5000/5000 [==============================] - 2s 351us/sample - loss: 1.8013 - acc: 0.5780\n",
      "1407/1407 [==============================] - 40s 29ms/step - loss: 1.0409 - acc: 0.7485 - val_loss: 1.8007 - val_acc: 0.5780\n",
      "Epoch 20/20\n",
      "5000/5000 [==============================] - 2s 348us/sample - loss: 1.6158 - acc: 0.6212\n",
      "1407/1407 [==============================] - 40s 29ms/step - loss: 1.0369 - acc: 0.7512 - val_loss: 1.6166 - val_acc: 0.6212\n",
      "Epoch 1/20\n",
      "5000/5000 [==============================] - 2s 401us/sample - loss: 1.1191 - acc: 0.7308\n",
      "1407/1407 [==============================] - 42s 30ms/step - loss: 1.0299 - acc: 0.7556 - val_loss: 1.1214 - val_acc: 0.7308\n",
      "Epoch 2/20\n",
      "5000/5000 [==============================] - 2s 345us/sample - loss: 1.0487 - acc: 0.7572\n",
      "1407/1407 [==============================] - 40s 29ms/step - loss: 1.0346 - acc: 0.7530 - val_loss: 1.0513 - val_acc: 0.7572\n",
      "Epoch 3/20\n",
      "5000/5000 [==============================] - 2s 354us/sample - loss: 1.0198 - acc: 0.7558\n",
      "1407/1407 [==============================] - 40s 29ms/step - loss: 1.0301 - acc: 0.7571 - val_loss: 1.0221 - val_acc: 0.7558\n",
      "Epoch 4/20\n",
      "5000/5000 [==============================] - 2s 351us/sample - loss: 1.4100 - acc: 0.6626\n",
      "1407/1407 [==============================] - 40s 29ms/step - loss: 1.0291 - acc: 0.7575 - val_loss: 1.4094 - val_acc: 0.6626\n",
      "Epoch 5/20\n",
      "5000/5000 [==============================] - 2s 332us/sample - loss: 1.5767 - acc: 0.6416\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 1.0216 - acc: 0.7609 - val_loss: 1.5765 - val_acc: 0.6416\n",
      "Epoch 6/20\n",
      "5000/5000 [==============================] - 2s 334us/sample - loss: 1.2204 - acc: 0.7014\n",
      "1407/1407 [==============================] - 40s 29ms/step - loss: 1.0243 - acc: 0.7604 - val_loss: 1.2213 - val_acc: 0.7014\n",
      "Epoch 7/20\n",
      "5000/5000 [==============================] - 2s 340us/sample - loss: 1.5776 - acc: 0.6482\n",
      "1407/1407 [==============================] - 40s 29ms/step - loss: 1.0220 - acc: 0.7620 - val_loss: 1.5758 - val_acc: 0.6482\n",
      "Epoch 8/20\n",
      "5000/5000 [==============================] - 2s 353us/sample - loss: 1.1045 - acc: 0.7342\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 1.0211 - acc: 0.7639 - val_loss: 1.1052 - val_acc: 0.7342\n",
      "Epoch 9/20\n",
      "5000/5000 [==============================] - 2s 325us/sample - loss: 1.1142 - acc: 0.7416\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 1.0168 - acc: 0.7676 - val_loss: 1.1169 - val_acc: 0.7416\n",
      "Epoch 10/20\n",
      "5000/5000 [==============================] - 2s 361us/sample - loss: 1.2381 - acc: 0.7182\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 1.0180 - acc: 0.7666 - val_loss: 1.2387 - val_acc: 0.7182\n",
      "Epoch 11/20\n",
      "5000/5000 [==============================] - 2s 353us/sample - loss: 1.3044 - acc: 0.7042\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 1.0160 - acc: 0.7686 - val_loss: 1.3047 - val_acc: 0.7042\n",
      "Epoch 12/20\n",
      "5000/5000 [==============================] - 2s 352us/sample - loss: 1.4765 - acc: 0.6680\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 1.0103 - acc: 0.7710 - val_loss: 1.4747 - val_acc: 0.6680\n",
      "Epoch 13/20\n",
      "5000/5000 [==============================] - 2s 345us/sample - loss: 1.4627 - acc: 0.6550\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 1.0062 - acc: 0.7730 - val_loss: 1.4617 - val_acc: 0.6550\n",
      "Epoch 14/20\n",
      "5000/5000 [==============================] - 2s 345us/sample - loss: 1.1757 - acc: 0.7198\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 1.0112 - acc: 0.7712 - val_loss: 1.1792 - val_acc: 0.7198\n",
      "Epoch 15/20\n",
      "5000/5000 [==============================] - 2s 344us/sample - loss: 1.3732 - acc: 0.6952\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 1.0134 - acc: 0.7730 - val_loss: 1.3738 - val_acc: 0.6952\n",
      "Epoch 16/20\n",
      "5000/5000 [==============================] - 2s 330us/sample - loss: 1.0552 - acc: 0.7558\n",
      "1407/1407 [==============================] - 40s 28ms/step - loss: 1.0071 - acc: 0.7737 - val_loss: 1.0586 - val_acc: 0.7558\n",
      "Epoch 17/20\n",
      "5000/5000 [==============================] - 2s 348us/sample - loss: 1.3849 - acc: 0.6618\n",
      "1407/1407 [==============================] - 40s 28ms/step - loss: 1.0020 - acc: 0.7731 - val_loss: 1.3878 - val_acc: 0.6618\n",
      "Epoch 18/20\n",
      "5000/5000 [==============================] - 2s 330us/sample - loss: 1.3143 - acc: 0.6918\n",
      "1407/1407 [==============================] - 40s 28ms/step - loss: 1.0066 - acc: 0.7743 - val_loss: 1.3171 - val_acc: 0.6918\n",
      "Epoch 19/20\n",
      "5000/5000 [==============================] - 2s 348us/sample - loss: 1.7325 - acc: 0.6132\n",
      "1407/1407 [==============================] - 40s 29ms/step - loss: 1.0064 - acc: 0.7732 - val_loss: 1.7324 - val_acc: 0.6132\n",
      "Epoch 20/20\n",
      "5000/5000 [==============================] - 2s 332us/sample - loss: 2.1659 - acc: 0.5374\n",
      "1407/1407 [==============================] - 40s 28ms/step - loss: 1.0161 - acc: 0.7729 - val_loss: 2.1690 - val_acc: 0.5374\n",
      "Epoch 1/20\n",
      "5000/5000 [==============================] - 2s 420us/sample - loss: 0.7865 - acc: 0.8480\n",
      "1407/1407 [==============================] - 42s 30ms/step - loss: 0.8254 - acc: 0.8339 - val_loss: 0.7882 - val_acc: 0.8480\n",
      "Epoch 2/20\n",
      "5000/5000 [==============================] - 2s 351us/sample - loss: 0.7883 - acc: 0.8424\n",
      "1407/1407 [==============================] - 40s 29ms/step - loss: 0.7507 - acc: 0.8536 - val_loss: 0.7905 - val_acc: 0.8424\n",
      "Epoch 3/20\n",
      "5000/5000 [==============================] - 2s 344us/sample - loss: 0.7557 - acc: 0.8508\n",
      "1407/1407 [==============================] - 40s 29ms/step - loss: 0.7074 - acc: 0.8612 - val_loss: 0.7580 - val_acc: 0.8508\n",
      "Epoch 4/20\n",
      "5000/5000 [==============================] - 2s 327us/sample - loss: 0.7201 - acc: 0.8564\n",
      "1407/1407 [==============================] - 40s 28ms/step - loss: 0.6778 - acc: 0.8668 - val_loss: 0.7234 - val_acc: 0.8564\n",
      "Epoch 5/20\n",
      "5000/5000 [==============================] - 2s 349us/sample - loss: 0.6911 - acc: 0.8614\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 0.6521 - acc: 0.8728 - val_loss: 0.6938 - val_acc: 0.8614\n",
      "Epoch 6/20\n",
      "5000/5000 [==============================] - 2s 351us/sample - loss: 0.6626 - acc: 0.8642\n",
      "1407/1407 [==============================] - 42s 30ms/step - loss: 0.6293 - acc: 0.8746 - val_loss: 0.6643 - val_acc: 0.8642\n",
      "Epoch 7/20\n",
      "5000/5000 [==============================] - 2s 330us/sample - loss: 0.6809 - acc: 0.8622\n",
      "1407/1407 [==============================] - 40s 29ms/step - loss: 0.6115 - acc: 0.8774 - val_loss: 0.6831 - val_acc: 0.8622\n",
      "Epoch 8/20\n",
      "5000/5000 [==============================] - 2s 357us/sample - loss: 0.6478 - acc: 0.8586\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 0.5907 - acc: 0.8813 - val_loss: 0.6516 - val_acc: 0.8586\n",
      "Epoch 9/20\n",
      "5000/5000 [==============================] - 2s 347us/sample - loss: 0.6754 - acc: 0.8492\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 0.5775 - acc: 0.8825 - val_loss: 0.6772 - val_acc: 0.8492\n",
      "Epoch 10/20\n",
      "5000/5000 [==============================] - 2s 360us/sample - loss: 0.6713 - acc: 0.8528\n",
      "1407/1407 [==============================] - 40s 29ms/step - loss: 0.5601 - acc: 0.8831 - val_loss: 0.6726 - val_acc: 0.8528\n",
      "Epoch 11/20\n",
      "5000/5000 [==============================] - 2s 359us/sample - loss: 0.6301 - acc: 0.8662\n",
      "1407/1407 [==============================] - 40s 29ms/step - loss: 0.5473 - acc: 0.8855 - val_loss: 0.6321 - val_acc: 0.8662\n",
      "Epoch 12/20\n",
      "5000/5000 [==============================] - 2s 328us/sample - loss: 0.6924 - acc: 0.8452\n",
      "1407/1407 [==============================] - 40s 28ms/step - loss: 0.5360 - acc: 0.8868 - val_loss: 0.6955 - val_acc: 0.8452\n",
      "Epoch 13/20\n",
      "5000/5000 [==============================] - 2s 348us/sample - loss: 0.5934 - acc: 0.8680\n",
      "1407/1407 [==============================] - 40s 28ms/step - loss: 0.5297 - acc: 0.8865 - val_loss: 0.5958 - val_acc: 0.8680\n",
      "Epoch 14/20\n",
      "5000/5000 [==============================] - 2s 357us/sample - loss: 0.6189 - acc: 0.8618\n",
      "1407/1407 [==============================] - 40s 28ms/step - loss: 0.5152 - acc: 0.8909 - val_loss: 0.6197 - val_acc: 0.8618\n",
      "Epoch 15/20\n",
      "5000/5000 [==============================] - 2s 341us/sample - loss: 0.5999 - acc: 0.8618\n",
      "1407/1407 [==============================] - 40s 29ms/step - loss: 0.5116 - acc: 0.8903 - val_loss: 0.6030 - val_acc: 0.8618\n",
      "Epoch 16/20\n",
      "5000/5000 [==============================] - 2s 329us/sample - loss: 0.5697 - acc: 0.8732\n",
      "1407/1407 [==============================] - 40s 29ms/step - loss: 0.5057 - acc: 0.8907 - val_loss: 0.5711 - val_acc: 0.8732\n",
      "Epoch 17/20\n",
      "5000/5000 [==============================] - 2s 358us/sample - loss: 0.6227 - acc: 0.8592\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 0.4991 - acc: 0.8916 - val_loss: 0.6231 - val_acc: 0.8592\n",
      "Epoch 18/20\n",
      "5000/5000 [==============================] - 2s 354us/sample - loss: 0.6143 - acc: 0.8568\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 0.4902 - acc: 0.8945 - val_loss: 0.6160 - val_acc: 0.8568\n",
      "Epoch 19/20\n",
      "5000/5000 [==============================] - 2s 349us/sample - loss: 0.6398 - acc: 0.8544\n",
      "1407/1407 [==============================] - 40s 29ms/step - loss: 0.4901 - acc: 0.8905 - val_loss: 0.6434 - val_acc: 0.8544\n",
      "Epoch 20/20\n",
      "5000/5000 [==============================] - 2s 347us/sample - loss: 0.6089 - acc: 0.8592\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 0.4836 - acc: 0.8950 - val_loss: 0.6095 - val_acc: 0.8592\n",
      "Epoch 1/20\n",
      "5000/5000 [==============================] - 2s 437us/sample - loss: 0.5273 - acc: 0.8864\n",
      "1407/1407 [==============================] - 43s 31ms/step - loss: 0.4367 - acc: 0.9089 - val_loss: 0.5283 - val_acc: 0.8864\n",
      "Epoch 2/20\n",
      "5000/5000 [==============================] - 2s 344us/sample - loss: 0.5165 - acc: 0.8900\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 0.4114 - acc: 0.9190 - val_loss: 0.5178 - val_acc: 0.8900\n",
      "Epoch 3/20\n",
      "5000/5000 [==============================] - 2s 349us/sample - loss: 0.5185 - acc: 0.8884\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 0.4054 - acc: 0.9193 - val_loss: 0.5197 - val_acc: 0.8884\n",
      "Epoch 4/20\n",
      "5000/5000 [==============================] - 2s 333us/sample - loss: 0.5352 - acc: 0.8878\n",
      "1407/1407 [==============================] - 39s 28ms/step - loss: 0.3989 - acc: 0.9215 - val_loss: 0.5354 - val_acc: 0.8878\n",
      "Epoch 5/20\n",
      "5000/5000 [==============================] - 2s 338us/sample - loss: 0.5244 - acc: 0.8888\n",
      "1407/1407 [==============================] - 39s 28ms/step - loss: 0.3927 - acc: 0.9228 - val_loss: 0.5252 - val_acc: 0.8888\n",
      "Epoch 6/20\n",
      "5000/5000 [==============================] - 2s 331us/sample - loss: 0.5099 - acc: 0.8926s -\n",
      "1407/1407 [==============================] - 39s 28ms/step - loss: 0.3872 - acc: 0.9244 - val_loss: 0.5108 - val_acc: 0.8926\n",
      "Epoch 7/20\n",
      "5000/5000 [==============================] - 2s 337us/sample - loss: 0.5105 - acc: 0.8934\n",
      "1407/1407 [==============================] - 39s 28ms/step - loss: 0.3820 - acc: 0.9266 - val_loss: 0.5113 - val_acc: 0.8934\n",
      "Epoch 8/20\n",
      "5000/5000 [==============================] - 2s 353us/sample - loss: 0.5145 - acc: 0.8906\n",
      "1407/1407 [==============================] - 40s 29ms/step - loss: 0.3814 - acc: 0.9253 - val_loss: 0.5150 - val_acc: 0.8906\n",
      "Epoch 9/20\n",
      "5000/5000 [==============================] - 2s 320us/sample - loss: 0.5079 - acc: 0.8930\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 0.3743 - acc: 0.9282 - val_loss: 0.5086 - val_acc: 0.8930\n",
      "Epoch 10/20\n",
      "5000/5000 [==============================] - 2s 350us/sample - loss: 0.5032 - acc: 0.8940\n",
      "1407/1407 [==============================] - 40s 28ms/step - loss: 0.3735 - acc: 0.9281 - val_loss: 0.5040 - val_acc: 0.8940\n",
      "Epoch 11/20\n",
      "5000/5000 [==============================] - 2s 307us/sample - loss: 0.5107 - acc: 0.8874\n",
      "1407/1407 [==============================] - 40s 29ms/step - loss: 0.3697 - acc: 0.9297 - val_loss: 0.5112 - val_acc: 0.8874\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 2s 342us/sample - loss: 0.5051 - acc: 0.8914\n",
      "1407/1407 [==============================] - 40s 28ms/step - loss: 0.3642 - acc: 0.9313 - val_loss: 0.5059 - val_acc: 0.8914\n",
      "Epoch 13/20\n",
      "5000/5000 [==============================] - 2s 359us/sample - loss: 0.4973 - acc: 0.8938\n",
      "1407/1407 [==============================] - 40s 29ms/step - loss: 0.3626 - acc: 0.9308 - val_loss: 0.4978 - val_acc: 0.8938\n",
      "Epoch 14/20\n",
      "5000/5000 [==============================] - 2s 344us/sample - loss: 0.5075 - acc: 0.8938\n",
      "1407/1407 [==============================] - 40s 28ms/step - loss: 0.3596 - acc: 0.9311 - val_loss: 0.5085 - val_acc: 0.8938\n",
      "Epoch 15/20\n",
      "5000/5000 [==============================] - 2s 343us/sample - loss: 0.5045 - acc: 0.8950\n",
      "1407/1407 [==============================] - 40s 28ms/step - loss: 0.3572 - acc: 0.9328 - val_loss: 0.5047 - val_acc: 0.8950\n",
      "Epoch 16/20\n",
      "5000/5000 [==============================] - 2s 349us/sample - loss: 0.5074 - acc: 0.8928\n",
      "1407/1407 [==============================] - 40s 29ms/step - loss: 0.3539 - acc: 0.9330 - val_loss: 0.5080 - val_acc: 0.8928\n",
      "Epoch 17/20\n",
      "5000/5000 [==============================] - 2s 368us/sample - loss: 0.5067 - acc: 0.8914\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 0.3514 - acc: 0.9341 - val_loss: 0.5076 - val_acc: 0.8914\n",
      "Epoch 18/20\n",
      "5000/5000 [==============================] - 2s 330us/sample - loss: 0.5081 - acc: 0.8910\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 0.3467 - acc: 0.9346 - val_loss: 0.5095 - val_acc: 0.8910\n",
      "Epoch 19/20\n",
      "5000/5000 [==============================] - 2s 361us/sample - loss: 0.5090 - acc: 0.8912\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 0.3471 - acc: 0.9354 - val_loss: 0.5097 - val_acc: 0.8912\n",
      "Epoch 20/20\n",
      "5000/5000 [==============================] - 2s 362us/sample - loss: 0.5030 - acc: 0.8884\n",
      "1407/1407 [==============================] - 40s 29ms/step - loss: 0.3465 - acc: 0.9347 - val_loss: 0.5041 - val_acc: 0.8884\n",
      "Epoch 1/20\n",
      "5000/5000 [==============================] - 2s 445us/sample - loss: 0.5143 - acc: 0.8872\n",
      "1407/1407 [==============================] - 43s 31ms/step - loss: 0.3413 - acc: 0.9373 - val_loss: 0.5151 - val_acc: 0.8872\n",
      "Epoch 2/20\n",
      "5000/5000 [==============================] - 2s 345us/sample - loss: 0.5031 - acc: 0.8920\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 0.3421 - acc: 0.9358 - val_loss: 0.5044 - val_acc: 0.8920\n",
      "Epoch 3/20\n",
      "5000/5000 [==============================] - 2s 346us/sample - loss: 0.5022 - acc: 0.8912\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 0.3380 - acc: 0.9366 - val_loss: 0.5027 - val_acc: 0.8912\n",
      "Epoch 4/20\n",
      "5000/5000 [==============================] - 2s 347us/sample - loss: 0.5057 - acc: 0.8898\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 0.3343 - acc: 0.9366 - val_loss: 0.5072 - val_acc: 0.8898\n",
      "Epoch 5/20\n",
      "5000/5000 [==============================] - 2s 350us/sample - loss: 0.4897 - acc: 0.8930\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 0.3330 - acc: 0.9379 - val_loss: 0.4906 - val_acc: 0.8930\n",
      "Epoch 6/20\n",
      "5000/5000 [==============================] - 2s 346us/sample - loss: 0.5170 - acc: 0.8900\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 0.3313 - acc: 0.9385 - val_loss: 0.5178 - val_acc: 0.8900\n",
      "Epoch 7/20\n",
      "5000/5000 [==============================] - 2s 352us/sample - loss: 0.4899 - acc: 0.8946\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 0.3296 - acc: 0.9402 - val_loss: 0.4912 - val_acc: 0.8946\n",
      "Epoch 8/20\n",
      "5000/5000 [==============================] - 2s 330us/sample - loss: 0.4965 - acc: 0.8904\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 0.3270 - acc: 0.9396 - val_loss: 0.4969 - val_acc: 0.8904\n",
      "Epoch 9/20\n",
      "5000/5000 [==============================] - 2s 348us/sample - loss: 0.4959 - acc: 0.8922\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 0.3253 - acc: 0.9405 - val_loss: 0.4964 - val_acc: 0.8922\n",
      "Epoch 10/20\n",
      "5000/5000 [==============================] - 2s 355us/sample - loss: 0.5018 - acc: 0.8924\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 0.3241 - acc: 0.9393 - val_loss: 0.5026 - val_acc: 0.8924\n",
      "Epoch 11/20\n",
      "5000/5000 [==============================] - 2s 344us/sample - loss: 0.5079 - acc: 0.8910\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 0.3171 - acc: 0.9424 - val_loss: 0.5088 - val_acc: 0.8910\n",
      "Epoch 12/20\n",
      "5000/5000 [==============================] - 2s 354us/sample - loss: 0.5104 - acc: 0.8902\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 0.3174 - acc: 0.9402 - val_loss: 0.5115 - val_acc: 0.8902\n",
      "Epoch 13/20\n",
      "5000/5000 [==============================] - 2s 348us/sample - loss: 0.5001 - acc: 0.8906\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 0.3193 - acc: 0.9414 - val_loss: 0.5012 - val_acc: 0.8906\n",
      "Epoch 14/20\n",
      "5000/5000 [==============================] - 2s 360us/sample - loss: 0.4985 - acc: 0.8900\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 0.3161 - acc: 0.9414 - val_loss: 0.4997 - val_acc: 0.8900\n",
      "Epoch 15/20\n",
      "5000/5000 [==============================] - 2s 344us/sample - loss: 0.4948 - acc: 0.8918\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 0.3155 - acc: 0.9418 - val_loss: 0.4957 - val_acc: 0.8918\n",
      "Epoch 16/20\n",
      "5000/5000 [==============================] - 2s 335us/sample - loss: 0.4932 - acc: 0.8912\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 0.3101 - acc: 0.9445 - val_loss: 0.4940 - val_acc: 0.8912\n",
      "Epoch 17/20\n",
      "5000/5000 [==============================] - 2s 336us/sample - loss: 0.5107 - acc: 0.8884\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 0.3090 - acc: 0.9428 - val_loss: 0.5120 - val_acc: 0.8884\n",
      "Epoch 18/20\n",
      "5000/5000 [==============================] - 2s 355us/sample - loss: 0.5032 - acc: 0.8904\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 0.3065 - acc: 0.9449 - val_loss: 0.5039 - val_acc: 0.8904\n",
      "Epoch 19/20\n",
      "5000/5000 [==============================] - 2s 357us/sample - loss: 0.5047 - acc: 0.8894\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 0.3080 - acc: 0.9436 - val_loss: 0.5055 - val_acc: 0.8894\n",
      "Epoch 20/20\n",
      "5000/5000 [==============================] - 2s 359us/sample - loss: 0.4888 - acc: 0.8930\n",
      "1407/1407 [==============================] - 40s 29ms/step - loss: 0.3038 - acc: 0.9435 - val_loss: 0.4898 - val_acc: 0.8930\n"
     ]
    }
   ],
   "source": [
    "lr_adjust = [0.1, 0.1, 0.01, 0.001, 0.001]\n",
    "for lr in lr_adjust:\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.SGD(lr=lr, momentum=0.9),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    model.fit_generator(datagen.flow(x_train, y_train, batch_size=32), steps_per_epoch=len(x_train)//32,\n",
    "                        validation_data=(x_dev, y_dev), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('ResNet32_acc893.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
