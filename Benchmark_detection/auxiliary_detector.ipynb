{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from utils import cifar10_load\n",
    "from models import build_ResNet\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "batch_size = 32\n",
    "training_epochs = 20\n",
    "DYNAMIC_ADVERSARY = False\n",
    "\n",
    "(x_train, y_train), (x_dev, y_dev), (x_test, y_test) = cifar10_load()\n",
    "# 这里用name_scope是无效的\n",
    "model = tf.keras.models.load_model('ResNet32_acc893.h5')\n",
    "base_varlist = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "sess = tf.keras.backend.get_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lr_init 0.1, 40epochs后调为0.01, 60epochs后调为0.001, 共100epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Activation\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# 这里name_scope是有效的\n",
    "detector = Sequential([\n",
    "    Conv2D(96, (3,3), strides=(1,1), padding='same', activation='relu', kernel_initializer='he_normal', input_shape=(32,32,16)),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(192, (3,3), strides=(1,1), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(192, (3,3), strides=(1,1), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "    Conv2D(2, (1,1), strides=(1,1), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "    GlobalAveragePooling2D()])\n",
    "detect_pipeline = Model(inputs=model.input, outputs=Activation('softmax', name=\"detect_softmax\")(detector(model.get_layer('activation_10').output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/cleverhans-master/cleverhans/attacks_tf.py:27: UserWarning: attacks_tf is deprecated and will be removed on 2019-07-18 or after. Code should import functions from their new locations directly.\n",
      "  warnings.warn(\"attacks_tf is deprecated and will be removed on 2019-07-18\"\n",
      "/tf/cleverhans-master/cleverhans/compat.py:23: UserWarning: <function reduce_sum_v1 at 0x7f9b054821e0> is deprecated. Switch to calling the equivalent function in tensorflow.  This function was originally needed as a compatibility layer for old versions of tensorflow,  but support for those versions has now been dropped.\n",
      "  warnings.warn(str(f) + \" is deprecated. Switch to calling the equivalent function in tensorflow. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tf/cleverhans-master/cleverhans/attacks/projected_gradient_descent.py:96: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/cleverhans-master/cleverhans/compat.py:23: UserWarning: <function reduce_max_v1 at 0x7f9b05482ea0> is deprecated. Switch to calling the equivalent function in tensorflow.  This function was originally needed as a compatibility layer for old versions of tensorflow,  but support for those versions has now been dropped.\n",
      "  warnings.warn(str(f) + \" is deprecated. Switch to calling the equivalent function in tensorflow. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tf/cleverhans-master/cleverhans/compat.py:80: calling softmax_cross_entropy_with_logits_v2_helper (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n"
     ]
    }
   ],
   "source": [
    "# Static adversaries.\n",
    "# Generate adversarail examples fool only networks\n",
    "from cleverhans.utils_keras import KerasModelWrapper\n",
    "import numpy as np\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(None,32,32,3))\n",
    "x_label = tf.placeholder(tf.float32, shape=(None,2))\n",
    "adv_label = tf.placeholder(tf.float32, shape=(None,2))\n",
    "\n",
    "model_wrap = KerasModelWrapper(model)\n",
    "detect_wrap = KerasModelWrapper(detect_pipeline)\n",
    "\n",
    "pgd_params = {'eps':0.3,\n",
    "              'eps_iter':0.25,\n",
    "              'nb_iter':10, \n",
    "              'ord':2,\n",
    "              'clip_min':0., \n",
    "              'clip_max':1., \n",
    "              'sanity_checks':True}\n",
    "\n",
    "if DYNAMIC_ADVERSARY:\n",
    "    from modified_PGD import ProjectedGradientDescent\n",
    "    pgd_params['y'] = adv_label\n",
    "    pgd_wrap = ProjectedGradientDescent(model_wrap, detect_wrap, sess=sess, default_rand_init=False)\n",
    "else:\n",
    "    from cleverhans.attacks import ProjectedGradientDescent\n",
    "    pgd_wrap = ProjectedGradientDescent(model_wrap, sess=sess, default_rand_init=False)\n",
    "\n",
    "\n",
    "\n",
    "adv_x = pgd_wrap.generate(x, **pgd_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "攻击时对每一个像素点以0.5的概率修改\n",
    "\n",
    "目的是提高泛化性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_mask = np.random.randint(2, size=(batch_size,32,32,1))\n",
    "adv_x = x*adv_mask + adv_x*(1-adv_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_labels(length):\n",
    "    adv_labels = np.zeros([length, 2])\n",
    "    normal_labels = np.copy(adv_labels)\n",
    "    adv_labels[:, 1] = 1 \n",
    "    normal_labels[:, 0] = 1\n",
    "    \n",
    "    return adv_labels, normal_labels\n",
    "\n",
    "def initialize_uninitialized(sess):\n",
    "       global_vars = tf.global_variables()\n",
    "       is_not_initialized = sess.run([tf.is_variable_initialized(var) for var in global_vars])\n",
    "       not_initialized_vars = [v for (v, f) in zip(global_vars, is_not_initialized) if not f]\n",
    "       #print([str(i.name) for i in not_initialized_vars]) # only for testing\n",
    "       if len(not_initialized_vars):\n",
    "            sess.run(tf.variables_initializer(not_initialized_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_varlist = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "new_varlist = list(set(total_varlist) - set(base_varlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/cleverhans-master/cleverhans/utils_keras.py:210: UserWarning: Abstract layer detected, picking last ouput node as default.This could happen due to using of stacked model.\n",
      "  warnings.warn(\"Abstract layer detected, picking last ouput node as default.\"\n"
     ]
    }
   ],
   "source": [
    "detect_x = detect_pipeline(x)\n",
    "detect_adv = detect_pipeline(adv_x)\n",
    "\n",
    "detect_x_logits = detect_wrap.get_logits(x)\n",
    "detect_adv_logits = detect_wrap.get_logits(adv_x)\n",
    "\n",
    "#loss1 = tf.reduce_sum(tf.keras.backend.categorical_crossentropy(detect_x_logits, x_label, from_logits=True))\n",
    "#loss2 = tf.reduce_sum(tf.keras.backend.categorical_crossentropy(detect_adv_logits, adv_label, from_logits=True))\n",
    "loss1 = tf.reduce_sum(tf.keras.backend.categorical_crossentropy(detect_x, x_label))\n",
    "loss2 = tf.reduce_sum(tf.keras.backend.categorical_crossentropy(detect_adv, adv_label))\n",
    "total_loss = loss1 + loss2\n",
    "#train_step1 = tf.train.GradientDescentOptimizer(learning_rate=1e-4).minimize(loss1, var_list=new_varlist)\n",
    "#train_step2 = tf.train.GradientDescentOptimizer(learning_rate=1e-4).minimize(loss2, var_list=new_varlist)\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=1e-4, beta1=0.99, beta2=0.999).minimize(total_loss)\n",
    "\n",
    "correct_prediction1 = tf.equal(tf.argmax(detect_x, 1), tf.argmax(x_label, 1))\n",
    "correct_prediction2 = tf.equal(tf.argmax(detect_adv, 1), tf.argmax(adv_label, 1))\n",
    "accuracy1 = tf.reduce_mean(tf.cast(correct_prediction1, \"float\"), name='accuracy1')\n",
    "accuracy2 = tf.reduce_mean(tf.cast(correct_prediction2, \"float\"), name='accuracy2')\n",
    "accuracy = 0.5*accuracy1 + 0.5*accuracy2\n",
    "\n",
    "initialize_uninitialized(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.summary.scalar('total_loss',total_loss)\n",
    "tf.summary.scalar('loss_clean', loss1)\n",
    "tf.summary.scalar('loss_adv', loss2)\n",
    "tf.summary.scalar('accuracy_clean', accuracy1)\n",
    "tf.summary.scalar('accuracy_adv', accuracy2)\n",
    "writer = tf.summary.FileWriter('/tflog',sess.graph)\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss1 = tf.reduce_sum(tf.keras.backend.categorical_crossentropy(detect_x_logits, x_label, from_logits=True))\n",
    "loss2 = tf.reduce_sum(tf.keras.backend.categorical_crossentropy(detect_adv_logits, adv_label, from_logits=True))\n",
    "train_step1 = tf.train.GradientDescentOptimizer(learning_rate=1e-4).minimize(loss1, var_list=new_varlist)\n",
    "train_step2 = tf.train.GradientDescentOptimizer(learning_rate=1e-4).minimize(loss2, var_list=new_varlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0, test accuracy: 0.5\n",
      "iteration: 1, test accuracy: 0.5\n",
      "iteration: 2, test accuracy: 0.5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-c225372231b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     test_accuracy=[accuracy.eval(session=sess, feed_dict={x: x_dev[j*batch_size:(j+1)*batch_size], \n\u001b[1;32m     10\u001b[0m                                                           \u001b[0mx_label\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnormal_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                                                           adv_label: adv_labels}) for j in range(len(x_dev)//batch_size-1)]\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"iteration: %d, test accuracy: %g\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-c225372231b2>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m     test_accuracy=[accuracy.eval(session=sess, feed_dict={x: x_dev[j*batch_size:(j+1)*batch_size], \n\u001b[1;32m     10\u001b[0m                                                           \u001b[0mx_label\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnormal_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                                                           adv_label: adv_labels}) for j in range(len(x_dev)//batch_size-1)]\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"iteration: %d, test accuracy: %g\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m     \"\"\"\n\u001b[0;32m--> 695\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   5179\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5180\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 5181\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(training_epochs):\n",
    "    adv_labels, normal_labels = generate_labels(batch_size)\n",
    "    for j in range(len(x_train)//batch_size-1):\n",
    "        _, tf_log = sess.run([train_step2, merged], feed_dict={x: x_train[j*batch_size:(j+1)*batch_size],\n",
    "                                                   x_label: normal_labels, adv_label: adv_labels})\n",
    "        writer.add_summary(tf_log,j+i*(len(x_train)//batch_size-1))\n",
    "    \n",
    "    #adv_labels, normal_labels = generate_labels(len(x_dev[:500]))\n",
    "    test_accuracy=[accuracy.eval(session=sess, feed_dict={x: x_dev[j*batch_size:(j+1)*batch_size], \n",
    "                                                          x_label: normal_labels, \n",
    "                                                          adv_label: adv_labels}) for j in range(len(x_dev)//batch_size-1)]\n",
    "    \n",
    "    print(\"iteration: %d, test accuracy: %g\" % (i, sum(test_accuracy)/float(len(test_accuracy))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DYNAMIC_ADVERSARY:\n",
    "    detect_pipeline.save('detector_dynamic.h5')\n",
    "else:\n",
    "    detect_pipeline.save('detector_static.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(training_epochs):\n",
    "    adv_labels, normal_labels = generate_labels(batch_size)\n",
    "    if i%2 == 0:\n",
    "        for j in range(len(x_train)//batch_size-1):\n",
    "            _, tf_log = sess.run([train_step2, merged], feed_dict={x: x_train[j*batch_size:(j+1)*batch_size],\n",
    "                                                   x_label: normal_labels, adv_label: adv_labels})\n",
    "            writer.add_summary(tf_log,j+i*(len(x_train)//batch_size-1))\n",
    "    else:\n",
    "        for j in range(len(x_train)//batch_size-1):\n",
    "            _, tf_log = sess.run([train_step1, merged], feed_dict={x: x_train[j*batch_size:(j+1)*batch_size],\n",
    "                                                   x_label: normal_labels, adv_label: adv_labels})\n",
    "            writer.add_summary(tf_log,j+i*(len(x_train)//batch_size-1))\n",
    "    adv_labels, normal_labels = generate_labels(len(x_dev[:500]))\n",
    "    print(\"iteration: %d, test accuracy: %g\" % (i, accuracy.eval(session=sess, feed_dict={x: x_dev[:500], \n",
    "                                                                                          x_label: normal_labels, \n",
    "                                                                                          adv_label: adv_labels})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dynamic adversaries.\n",
    "# Generate adversarail examples fool both network and classifier\n",
    "from cleverhans.utils_keras import KerasModelWrapper\n",
    "from modified_PGD import ProjectedGradientDescent\n",
    "import numpy as np\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(None,32,32,3))\n",
    "x_label = tf.placeholder(tf.float32, shape=(None,2))\n",
    "adv_label = tf.placeholder(tf.float32, shape=(None,2))\n",
    "\n",
    "model_wrap = KerasModelWrapper(model)\n",
    "detect_wrap = KerasModelWrapper(detect_pipeline)\n",
    "pgd_wrap = ProjectedGradientDescent(model_wrap, detect_wrap, sess=sess, default_rand_init=False)\n",
    "\n",
    "pgd_params = {'eps':0.3,\n",
    "              'eps_iter':0.25,\n",
    "              'y':adv_label,\n",
    "              'nb_iter':10, \n",
    "              'ord':np.inf,\n",
    "              'clip_min':0., \n",
    "              'clip_max':1., \n",
    "              'sanity_checks':True}\n",
    "\n",
    "adv_x = pgd_wrap.generate(x, **pgd_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_labels, normal_labels = generate_labels(1)\n",
    "t1,t2 = sess.run([adv_x,x], feed_dict={x: x_train[0].reshape(-1,32,32,3), x_label: normal_labels, adv_label: adv_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_pipeline.predict(t1.reshape(-1,32,32,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_pipeline.predict(t2.reshape(-1,32,32,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3,t4 = sess.run([detect_x_logits,detect_adv_logits], feed_dict={x: x_train[0].reshape(-1,32,32,3), x_label: normal_labels, adv_label: adv_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "adv_labels, normal_labels = generate_labels(batch_size)\n",
    "adv_images = adv_x.eval(session=sess, feed_dict={x: x_train[j*batch_size:(j+1)*batch_size],\n",
    "                                               x_label: normal_labels, adv_label: adv_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(adv_images[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(x_train[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.predict(x_train[3].reshape(-1,32,32,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.predict(adv_images[3].reshape(-1,32,32,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlayers = [layer.get_output_at(-1) for layer in detect_pipeline.layers]\n",
    "test = Model(inputs=detect_pipeline.input, outputs=outlayers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将Dectector和原来model的隐藏层连起来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "detect_pipeline.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleverhans.utils_keras import KerasModelWrapper\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(None,32,32,3))\n",
    "y = tf.placeholder(tf.float32, shape=(None,10))\n",
    "model_wrap = KerasModelWrapper(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Activation\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "hidden = model.get_layer('activation_10').output\n",
    "\n",
    "conv1 = Conv2D(96, (3,3), strides=(1,1), padding='same', activation='relu', kernel_initializer='uniform',\n",
    "              name='detector_conv1')(hidden)\n",
    "mp1 = MaxPooling2D(name='detector_mp1')(conv1)\n",
    "conv2 = Conv2D(192, (3,3), strides=(1,1), padding='same', activation='relu', kernel_initializer='uniform',\n",
    "              name='detector_conv2')(mp1)\n",
    "mp2 = MaxPooling2D(name='detector_mp2')(conv2)\n",
    "conv3 = Conv2D(192, (3,3), strides=(1,1), padding='same', activation='relu', kernel_initializer='uniform',\n",
    "              name='detector_conv3')(mp2)\n",
    "conv4 = Conv2D(2, (1,1), strides=(1,1), padding='same', activation='relu', kernel_initializer='uniform',\n",
    "              name='detector_conv4')(conv3)\n",
    "GAP = GlobalAveragePooling2D(name='detector_GAP')(conv4)\n",
    "\n",
    "detector_out = Activation('softmax', name='detector_softmax')(GAP)\n",
    "detect_pipeline = Model(inputs=model.input, outputs=detector_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "detect_pipeline = Model(inputs=model.input, outputs=Activation('softmax', name=\"detect_softmax\")(detector(model.get_layer('add_3').output)))\n",
    "detect_wrap = KerasModelWrapper(detect_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "detect_pipeline.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import cifar10_load\n",
    "(x_train, y_train), (x_dev, y_dev), (x_test, y_test) = cifar10_load()\n",
    "import numpy as np\n",
    "detect_pipeline.predict(x_dev[0].reshape(-1,32,32,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_pipeline.save('pipeline_test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "detect_pipeline = tf.keras.models.load_model('pipeline_test.h5')\n",
    "outlayers = [layer.output for layer in detect_pipeline.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlayers[31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Model(inputs=detect_pipeline.input, outputs=outlayers[32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检测器对对抗样本输出为1,正常样本输出为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from modified_PGD import ProjectedGradientDescent\n",
    "import numpy as np\n",
    "\n",
    "pgd_wrap = ProjectedGradientDescent(model_wrap, detect_wrap)\n",
    "pgd_params = {'eps':0.3, 'eps_iter':0.25, 'nb_iter':10, 'y':np.array([[0, 1]]), 'ord':np.inf,\n",
    "              'clip_min':0, 'clip_max':1, 'y_target':None, 'rand_init':None, 'rand_minmax':0.3, 'sanity_checks':True}\n",
    "adv_x = pgd_wrap.generate(x, **pgd_params)\n",
    "\n",
    "#train_pipeline = Model(inputs=x, outputs=[model(x), model(adv_x), detect_pipeline(x), detect_pipline(adv_x)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train pipeline没有办法抽象成Model储存，只能手写训练流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = model_wrap.get_layer(x, 'activation_10')\n",
    "detector_out = detector(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "detector.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "detect_pipeline.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('base_model'):\n",
    "    model = build_ResNet(depth=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_adjust = [0.1, 0.1, 0.01, 0.001, 0.001]\n",
    "for lr in lr_adjust:\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.SGD(lr=lr, momentum=0.9),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    model.fit_generator(datagen.flow(x_train, y_train, batch_size=32), steps_per_epoch=len(x_train)//32,\n",
    "                        validation_data=(x_dev, y_dev), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('ResNet32_acc893.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
